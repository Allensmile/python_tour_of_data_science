{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Python Tour of Data Science\n",
    "\n",
    "[MichaÃ«l Defferrard](http://deff.ch), *PhD student*, [EPFL](http://epfl.ch) [LTS2](http://lts2.epfl.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this short primer is to introduce the [Python](https://www.python.org) stack for [Data Science](https://en.wikipedia.org/wiki/Data_science). It is designed as a tour around the major Python packages used for the main computational tasks encountered in [the sexiest job of the 21st century](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century). Before starting, two remarks:\n",
    "1. There exists better / faster ways to accomplish the presented computations. The goal is to present the packages.\n",
    "1. It is not meant to teach you (scientific) Python. I however tried to include the main constructions and idioms of the language and packages. A good ressource to learn scientific Python is a [set of lectures](https://github.com/jrjohansson/scientific-python-lectures) from is J.R. Janson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"https://upload.wikimedia.org/wikipedia/commons/b/ba/Data_visualization_process_v1.png\" style=\"margin: 0px 0px 5px 20px; width: 600px; float: right;\">\n",
    "\n",
    "This notebook will walk you through a typical Data Science **process**:\n",
    "1. **Data acquisition**\n",
    "    1. Importation\n",
    "    1. Cleaning\n",
    "    1. Exploration\n",
    "1. **Data exploitation**\n",
    "    1. Pre-processing\n",
    "    1. (Feature extraction)\n",
    "    1. Models / Algorithms\n",
    "    1. Evaluation\n",
    "\n",
    "A **motivating example**: predict whether a credit card client will default.\n",
    "* Binary classification task: client will default or not ($y=1$ if yes; $y=0$ if no).\n",
    "* 30'000 real clients from Taiwan.\n",
    "* 23 numerical & categorical explanatory variables:\n",
    "    1. $x_1$: amount of the given credit.\n",
    "    2. $x_2$: gender (1 = male; 2 = female).\n",
    "    3. $x_3$: education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n",
    "    4. $x_4$: marital status (1 = married; 2 = single; 3 = others).\n",
    "    5. $x_5$: age (year).\n",
    "    6. $x_6$ to $x_{11}$: history of past payment (monthly from September to April, 2005) (-1 = pay duly; 1 = payment delay for one month; ...; 9 = payment delay for nine months and above).\n",
    "    7. $x_{12}$ to $x_{17}$: amount of bill statement (monthly from September to April, 2005).\n",
    "    8. $x_{18}$ to $x_{23}$: amount of previous payment (monthly from September to April, 2005).\n",
    "* Data downloaded from the [UCI ML repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this tour, we'll need the packages shown below, which are best installed from [PyPI](https://pypi.python.org) in a [virtual environment](https://docs.python.org/3/library/venv.html):\n",
    "```\n",
    "pyvenv /path/to/new/virtual/env\n",
    ". /path/to/new/virtual/env/bin/activate\n",
    "pip install -r requirements.txt\n",
    "jupyter-notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%script sh\n",
    "cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statements starting with `%` or `%%` are [built-in magic commands](http://ipython.readthedocs.io/en/stable/interactive/magics.html), i.e. commands interpreted by the IPython kernel. E.g. `%%script sh` tells IPython to run the cell with `sh` (like the `#!` line at the beginning of script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# While packages are usually imported at the top, they can\n",
    "# be imported wherever you prefer, in whatever scope.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data Importation\n",
    "\n",
    "* The world is messy, we got data in CSV, [JSON](http://www.json.org), Excel, [HDF5](https://www.hdfgroup.org/HDF5) files and an SQL database.\n",
    "* Could also have been matlab, HTML, XML files or from the web via scraping and APIs (e.g. [Twitter Firhose](https://dev.twitter.com/streaming/firehose)) or noSQL data stores, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the data.\n",
    "import utils\n",
    "utils.get_data('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Importing from an SQL Database\n",
    "\n",
    "[SQLAlchemy](http://www.sqlalchemy.org/) to the rescue.\n",
    "* Abstraction between DBAPIs.\n",
    "    * Supported databases: SQLite, Postgresql, MySQL, Oracle, MS-SQL, Firebird, Sybase and others.\n",
    "* [SQL Expression Language](http://docs.sqlalchemy.org/en/rel_1_0/core/tutorial.html).\n",
    "* [Object Relational Mapper (ORM)](http://docs.sqlalchemy.org/en/rel_1_0/orm/tutorial.html).\n",
    "\n",
    "TODO: show some ORM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "engine = sqlalchemy.create_engine('sqlite:///data/payments.sqlite', echo=False)\n",
    "\n",
    "# Infer from existing DB.\n",
    "metadata = sqlalchemy.MetaData()\n",
    "metadata.reflect(engine)\n",
    "\n",
    "# An SQL SELECT statement.\n",
    "table = metadata.tables.get('payments')\n",
    "op = sqlalchemy.sql.select([table])\n",
    "engine.echo = True\n",
    "result = engine.execute(op)\n",
    "engine.echo = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show some lines, i.e. clients.\n",
    "for row in result.fetchmany(size=10):\n",
    "    print('ID: {:2d}, payments: {}'.format(row[0], row[1:]))\n",
    "result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute some raw SQL.\n",
    "paid = 1000\n",
    "op = sqlalchemy.sql.text('SELECT payments.\"ID\", payments.\"PAY6\" FROM payments WHERE payments.\"PAY6\" = {}'.format(paid))\n",
    "result = engine.execute(op).fetchall()\n",
    "print('{} clients paid {} in April 2005'.format(len(result), paid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Merging data Sources\n",
    "\n",
    "Put some [pandas](http://pandas.pydata.org/) in our Python !\n",
    "* Import / export data from / to various sources.\n",
    "* Data frames manipulations: slicing, dicing, grouping.\n",
    "* And many more !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data(directory):\n",
    "    demographics = pd.read_csv(directory + 'demographics.csv', index_col=0)\n",
    "    delays = pd.read_excel(directory + 'delays.xls', index_col=0)\n",
    "    bills = pd.read_hdf(directory + 'bills.hdf5', 'bills')\n",
    "    payments = pd.read_sql('payments', engine, index_col='ID')\n",
    "    target = pd.read_json(directory + 'target.json')\n",
    "    return pd.concat([demographics, delays, bills, payments, target], axis=1)\n",
    "\n",
    "import pandas as pd\n",
    "data = get_data('data/')\n",
    "attributes = data.columns.tolist()\n",
    "\n",
    "# Tansform from numerical to categorical variable.\n",
    "data['SEX'] = data['SEX'].astype('category')\n",
    "data['SEX'].cat.categories = ['MALE', 'FEMALE']\n",
    "data['MARRIAGE'] = data['MARRIAGE'].astype('category')\n",
    "data['MARRIAGE'].cat.categories = ['UNK', 'MARRIED', 'SINGLE', 'OTHERS']\n",
    "data['EDUCATION'] = data['EDUCATION'].astype('category')\n",
    "data['EDUCATION'].cat.categories = ['UNK', 'GRAD SCHOOL', 'UNIVERSITY', 'HIGH SCHOOL', 'OTHERS', 'UNK1', 'UNK2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Looking at the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[:6, ['LIMIT', 'SEX', 'EDUCATION', 'MARRIAGE', 'AGE', 'DEFAULT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.iloc[:5, 4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.iloc[:5, 11:23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export as an [HTML table](./subset.html) for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data[:1000].to_html('subset.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data Cleaning\n",
    "\n",
    "While cleaning data is the [most time-consuming, least enjoyable Data Science task](http://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says), it should be perfomed nonetheless. Problems come in two flavours:\n",
    "\n",
    "1. Missing data, i.e. unknown values.\n",
    "1. Errors in data, i.e. wrong values.\n",
    "\n",
    "The actions to be taken in each case is highly **data and problem specific**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: marital status\n",
    "1. According to dataset description, it should either be 1 (married), 2 (single) or 3 (others).\n",
    "1. But we find some 0 (previously transformed to `UNK`).\n",
    "1. Let's *assume* that 0 represents errors when collecting the data and that we should remove those clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data['MARRIAGE'].value_counts())\n",
    "data = data[data['MARRIAGE'] != 'UNK']\n",
    "data['MARRIAGE'] = data['MARRIAGE'].cat.remove_unused_categories()\n",
    "print('\\nWe are left with {} clients\\n'.format(data.shape))\n",
    "print(data['MARRIAGE'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: education\n",
    "1. It should either be 1 (graduate school), 2 (university), 3 (high school) or 4 (others).\n",
    "1. But we find some 0, 5 and 6 (previously transformed to `UNK`, `UNK1` and `UNK2`).\n",
    "1. Let's *assume* these values are dubious, but do not invalidate the data and keep them as they may have some predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(data['EDUCATION'].value_counts())\n",
    "data.loc[data['EDUCATION']=='UNK1', 'EDUCATION'] = 'UNK'\n",
    "data.loc[data['EDUCATION']=='UNK2', 'EDUCATION'] = 'UNK'\n",
    "data['EDUCATION'] = data['EDUCATION'].cat.remove_unused_categories()\n",
    "print(data['EDUCATION'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Data Exploration\n",
    "\n",
    "* Get descriptive statistics.\n",
    "* Plot informative figures.\n",
    "* Verify some intuitive correlations.\n",
    "\n",
    "Let's get first some descriptive statistics of our numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attributes_numerical = ['LIMIT', 'AGE']\n",
    "attributes_numerical.extend(attributes[11:23])\n",
    "data.loc[:, attributes_numerical].describe().astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot an histogram of the ages, so that we get a better impression of who our clients are. That may even be an end goal, e.g. if your marketing team asks which customer groups to target.\n",
    "\n",
    "Then a boxplot of the bills, which may serve as a verification of the quality of the acquired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.loc[:, 'AGE'].plot.hist(bins=20, figsize=(15,5))\n",
    "ax = data.iloc[:, 11:17].plot.box(logy=True, figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple **question**: which proportion of our clients default ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "percentage = data['DEFAULT'].value_counts()[1] / data.shape[0] * 100\n",
    "print('Percentage of defaults: {:.2f}%'.format(percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another **question**: who's more susceptible to default, males or females ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observed = pd.crosstab(data['SEX'], data['DEFAULT'], margins=True)\n",
    "observed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like females are better risk. Let's verify with a Chi-Squared test of independance, using [scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "_, p, _, expected = stats.chi2_contingency(observed.iloc[:2,:2])\n",
    "print('p-value = {:.2e}'.format(p))\n",
    "print('expected values:\\n{}'.format(expected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intuition**: people who pay late present a higher risk of defaulting. Let's verify !\n",
    "Verifying some intuitions will also help you to identify mistakes. E.g. it would be suspicious if that intuition is not verified in the data: did we select the right column, or did we miss-compute a result ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "group = data.groupby('DELAY1').mean()\n",
    "corr = data['DEFAULT'].corr(data['DELAY1'], method='pearson')\n",
    "group['DEFAULT'].plot(grid=True, title='Pearson correlation: {:.4f}'.format(corr), figsize=(15,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Interactive Visualization\n",
    "\n",
    "[Bokeh](http://bokeh.pydata.org) is a Python interactive visualization library that targets modern web browsers for presentation, in the style of [D3.js](https://d3js.org). Alternatively, [matplotlib.widgets](http://matplotlib.org/api/widgets_api.html) could be used. Those interactive visualizations are very helpful to explore the data at hand in the quest of anomalies or patterns. Try with the plots below !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.plotting import output_notebook, figure, show\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "x, y1, y2 = 'LIMIT', 'PAY1', 'PAY2'\n",
    "n = 1000  # Less intensive for the browser.\n",
    "\n",
    "options = dict(\n",
    "    tools='pan,box_zoom,wheel_zoom,box_select,lasso_select,crosshair,reset,save',\n",
    "    x_axis_type='log', y_axis_type='log',\n",
    ")\n",
    "plot1 = figure(\n",
    "    x_range=[1e4,1e6],\n",
    "    x_axis_label=x, y_axis_label=y1,\n",
    "    **options\n",
    ")\n",
    "plot2 = figure(\n",
    "    x_range=plot1.x_range, y_range=plot1.y_range,\n",
    "    x_axis_label=x, y_axis_label=y2,\n",
    "    **options\n",
    ")\n",
    "\n",
    "html_color = lambda r,g,b: '#{:02x}{:02x}{:02x}'.format(r,g,b)\n",
    "colors = [html_color(150,0,0) if default == 1 else html_color(0,150,0) for default in data['DEFAULT'][:n]]\n",
    "# The above line is a list comprehension.\n",
    "\n",
    "radii = data['AGE'][:n] / 5\n",
    "\n",
    "# To link brushing (where a selection on one plot causes a selection to update on other plots).\n",
    "source = ColumnDataSource(data=dict(x=data[x][:n], y1=data[y1][:n], y2=data[y2][:n]))\n",
    "\n",
    "plot1.scatter('x', 'y1', source=source, size=radii, color=colors, alpha=0.6)\n",
    "plot2.scatter('x', 'y2', source=source, size=radii, color=colors, alpha=0.6)\n",
    "\n",
    "plot = gridplot([[plot1, plot2]], toolbar_location='right', plot_width=400, plot_height=400, title='adsf')\n",
    "\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Pre-Processing\n",
    "\n",
    "Back to [NumPy](http://www.numpy.org/), the fundamental package for scientific computing with Python. It provides multi-dimensional arrays, data types and linear algebra routines. Note that [scikit-learn](http://scikit-learn.org) provides many helpers for those tasks.\n",
    "\n",
    "Pre-processing usually consists of:\n",
    "1. Data types transformation. The data has not necessarilly the format the chosen learning algorithm expects.\n",
    "1. Data normalization. Some algorithms expect data to be centered and scaled. Some will train faster.\n",
    "1. Data randomization. If the samples are presented in sequence, it'll train faster if they are not correlated.\n",
    "1. Train / test splitting. You may have to be careful here, e.g. not including future events in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Back to numeric values.\n",
    "# Note: in a serious project, these should be treated as categories.\n",
    "data['SEX'].cat.categories = [-1, 1]\n",
    "data['SEX'] = data['SEX'].astype(np.int)\n",
    "data['MARRIAGE'].cat.categories = [-1, 1, 0]\n",
    "data['MARRIAGE'] = data['MARRIAGE'].astype(np.int)\n",
    "data['EDUCATION'].cat.categories = [-2, 2, 1, 0, -1]\n",
    "data['EDUCATION'] = data['EDUCATION'].astype(np.int)\n",
    "\n",
    "data['DEFAULT'] = data['DEFAULT'] * 2 - 1  # [0,1] --> [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Observations and targets.\n",
    "X = data.values[:,:23]\n",
    "y = data.values[:,23]\n",
    "n, d = X.shape\n",
    "print('The data is a {} with {} samples of dimensionality {}.'.format(type(X), n, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Center and scale.\n",
    "# Note: on a serious project, should be done after train / test split.\n",
    "X = X.astype(np.float)\n",
    "X -= X.mean(axis=0)\n",
    "X /= X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training and testing sets.\n",
    "test_size = 10000\n",
    "print('Split: {} testing and {} training samples'.format(test_size, y.size - test_size))\n",
    "perm = np.random.permutation(y.size)\n",
    "X_test  = X[perm[:test_size]]\n",
    "X_train = X[perm[test_size:]]\n",
    "y_test  = y[perm[:test_size]]\n",
    "y_train = y[perm[test_size:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 A first Predictive Model\n",
    "\n",
    "The ingredients of a Machine Learning (ML) model are:\n",
    "1. A predictive function, e.g. the linear transformation $f(x) = x^Tw + b$.\n",
    "1. An error function, e.g. the least squares $E = \\sum_{i=1}^n \\left( f(x_i) - y_i \\right)^2 = \\| f(X) - y \\|_2^2$.\n",
    "1. An optional regularization, e.g. the Thikonov regularization $R = \\|w\\|_2^2$.\n",
    "1. Which makes up the loss / objective function $L = E + \\alpha R$.\n",
    "\n",
    "Our model has a sole hyper-parameter, $\\alpha \\geq 0$, which controls the shrinkage.\n",
    "\n",
    "A Machine Learning (ML) problem can often be cast as a (convex or smooth) optimization problem which objective is to find the parameters (here $w$ and $b$) who minimize the loss, e.g.\n",
    "$$\\hat{w}, \\hat{b} = \\operatorname*{arg min}_{w,b} L = \\operatorname*{arg min}_{w,b} \\| Xw + b - y \\|_2^2 + \\alpha \\|w\\|_2^2.$$\n",
    "\n",
    "If the problem is convex and smooth, one can compute the gradients\n",
    "$$\\frac{\\partial L}{\\partial{w}} = 2 X^T (Xw+b-y) + 2\\alpha w,$$\n",
    "$$\\frac{\\partial L}{\\partial{b}} = 2 \\sum_{i=1}^n (x_i^Tw+b-y_i) = 2 \\sum_{i=1}^n (x_i^Tw-y_i) + 2n \\cdot b,$$\n",
    "\n",
    "which can be used in a [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) scheme or to form closed-form solutions:\n",
    "$$\\frac{\\partial L}{\\partial{w}} = 0 \\ \\rightarrow \\ 2 X^T X\\hat{w} + 2\\alpha \\hat{w} = 2 X^T y - 2 X^T b \\ \\rightarrow \\ \\hat{w} = (X^T X + \\alpha I)^{-1} X^T (y-b),$$\n",
    "$$\\frac{\\partial L}{\\partial{b}} = 0 \\ \\rightarrow \\ 2n\\hat{b} = 2\\sum_{i=1}^n (y_i) - \\underbrace{2\\sum_{i=1}^n (x_i^Tw)}_{=0 \\text{ if centered}} \\ \\rightarrow \\ \\hat{b} = \\frac1n I^T y = \\operatorname{mean}(y).$$\n",
    "\n",
    "What if the resulting problem is non-smooth ? See the [PyUNLocBoX](http://pyunlocbox.readthedocs.io), a convex optimization toolbox which implements [proximal splitting methods](https://en.wikipedia.org/wiki/Proximal_gradient_method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Take a *symbolic* Derivative\n",
    "\n",
    "Let's verify our manually derived gradients ! [SymPy](http://www.sympy.org/) is our computer algebra system (CAS) (like [Mathematica](https://www.wolfram.com/mathematica), [Maple](https://www.maplesoft.com/products/Maple)) of choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "X, y, w, b, a = sp.symbols('x y w b a')\n",
    "L = (X*w + b - y)**2 + a*w**2\n",
    "\n",
    "dLdw = sp.diff(L, w)\n",
    "dLdb = sp.diff(L, b)\n",
    "\n",
    "print('L = {}'.format(L))\n",
    "print('  dL/dw = {}'.format(dLdw))\n",
    "print('  dL/db = {}'.format(dLdb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Build the Classifier\n",
    "\n",
    "Relying on the derived equations, we can implement our model relying only on the [NumPy](http://www.numpy.org/) linear algebra capabilities (really wrappers to [BLAS](http://www.netlib.org/blas) / [LAPACK](http://www.netlib.org/lapack) implementations such as [ATLAS](http://math-atlas.sourceforge.net), [OpenBLAS](http://www.openblas.net) or [MKL](https://software.intel.com/intel-mkl)).\n",
    "\n",
    "A ML model is best represented as a class, with hyper-parameters and parameters stored as attributes, and is composed of two essential methods:\n",
    "1. `y_pred = model.predict(X_test)`: return the predictions $y$ given the features $X$.\n",
    "1. `model.fit(X_train, y_train)`: learn the model parameters such as to predict $y$ given $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RidgeRegression(object):\n",
    "    \"\"\"Our ML model.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0):\n",
    "        \"The class' constructor. Initialize the hyper-parameters.\"\n",
    "        self.a = alpha\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the predicted class given the features.\"\"\"\n",
    "        return np.sign(X.dot(self.w) + self.b)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Learn the model's parameters given the training data, the closed-form way.\"\"\"\n",
    "        n, d = X.shape\n",
    "        self.b = np.mean(y)\n",
    "        Ainv = np.linalg.inv(X.T.dot(X) + self.a * np.identity(d))\n",
    "        self.w = Ainv.dot(X.T).dot(y - self.b)\n",
    "\n",
    "    def loss(self, X, y, w=None, b=None):\n",
    "        \"\"\"Return the current loss.\n",
    "        This method is not strictly necessary, but it provides\n",
    "        information on the convergence of the learning process.\"\"\"\n",
    "        w = self.w if w is None else w  # The ternary conditional operator\n",
    "        b = self.b if b is None else b  # makes those tests concise.\n",
    "        import autograd.numpy as np  # See below for autograd.\n",
    "        return np.linalg.norm(np.dot(X, w) + b - y)**2 + self.a * np.linalg.norm(w, 2)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our model can learn its parameters and predict targets, it's time to evaluate it. Our metric for binary classification is the accuracy, which gives the percentage of correcly classified test samples. Depending on the application, the time spent for inference or training might also be important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Our evaluation metric, the classification accuracy.\"\"\"\n",
    "    return np.sum(y_pred == y_true) / y_true.size\n",
    "\n",
    "def evaluate(model):\n",
    "    \"\"\"Helper function to instantiate, train and evaluate the model.\n",
    "    It returns the classification accuracy, the loss and the execution time.\"\"\"\n",
    "    import time\n",
    "    t = time.process_time()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy(y_pred, y_test)\n",
    "    loss = model.loss(X_test, y_test)\n",
    "    t = time.process_time() - t\n",
    "    print('accuracy: {:.2f}%, loss: {:.2f}, time: {:.2f}ms'.format(acc*100, loss, t*1000))\n",
    "    return model\n",
    "\n",
    "alpha = 1e-2*n\n",
    "model = RidgeRegression(alpha)\n",
    "evaluate(model)\n",
    "\n",
    "models = []\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay we got around 80% accuracy with such a simple model ! Inference and training time looks good.\n",
    "\n",
    "For those of you who don't now about numerical mathematics, solving a linear system of equations by inverting a matrix can be numerically instable. Let's do it the proper way and use a proper solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_lapack(self, X, y):\n",
    "    \"\"\"Better way (numerical stability): solve the linear system with LAPACK.\"\"\"\n",
    "    n, d = X.shape\n",
    "    self.b = np.mean(y)\n",
    "    A = X.T.dot(X) + self.a * np.identity(d)\n",
    "    b = X.T.dot(y - self.b)\n",
    "    self.w = np.linalg.solve(A, b)\n",
    "\n",
    "# Let's monkey patch our object (Python is a dynamic language).\n",
    "RidgeRegression.fit = fit_lapack\n",
    "\n",
    "# Yeah just to be sure.\n",
    "models.append(evaluate(RidgeRegression(alpha)))\n",
    "assert np.allclose(models[-1].w, models[0].w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Learning as Gradient Descent\n",
    "\n",
    "Descending the gradient of our objective will lead us to a local minimum. If the objective is convex, that minimum will be global. Let's implement the gradient computed above and a simple gradient descent algorithm\n",
    "$$w^{(t+1)} = w^{(t)} - \\gamma \\frac{\\partial L}{\\partial w}$$\n",
    "where $\\gamma$ is the learning rate, another hyper-parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RidgeRegressionGradient(RidgeRegression):\n",
    "    \"\"\"This model inherits from `ridge_regression`. We overload the constructor, add a gradient\n",
    "    function and replace the learning algorithm, but don't touch the prediction and loss functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0, rate=0.1, niter=1000):\n",
    "        \"\"\"Here are new hyper-parameters: the learning rate and the number of iterations.\"\"\"\n",
    "        super().__init__(alpha)\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "        \n",
    "    def grad(self, X, y, w):\n",
    "        A = X.dot(w) + self.b - y\n",
    "        return 2 * X.T.dot(A) + 2 * self.a * w\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        self.b = np.mean(y)\n",
    "        \n",
    "        self.w = np.random.normal(size=d)\n",
    "        for i in range(self.niter):\n",
    "            self.w -= self.rate * self.grad(X, y, self.w)\n",
    "            \n",
    "            # Show convergence.\n",
    "            if i % (self.niter//10) == 0:\n",
    "                print('loss at iteration {}: {:.2f}'.format(i, self.loss(X, y)))\n",
    "            \n",
    "models.append(evaluate(RidgeRegressionGradient(alpha, 1e-6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tyred of derivating gradients by hand ? Welcome [autograd](https://github.com/HIPS/autograd/), our tool of choice for [automatic differentation](https://en.wikipedia.org/wiki/Automatic_differentiation). Alternatives are [Theano](http://deeplearning.net/software/theano/) and [TensorFlow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RidgeRegressionAutograd(RidgeRegressionGradient):\n",
    "    \"\"\"Here we derive the gradient during construction and update the gradient function.\"\"\"\n",
    "    def __init__(self, *args):\n",
    "        super().__init__(*args)\n",
    "        from autograd import grad\n",
    "        self.grad = grad(self.loss, argnum=2)\n",
    "\n",
    "models.append(evaluate(RidgeRegressionAutograd(alpha, 1e-6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Learning as generic Optimization\n",
    "\n",
    "Sometimes we don't want to implement the optimization by hand and would prefer a generic optimization algorithm. Let's make use of [SciPy](https://www.scipy.org/), which provides high-level algorithms for, e.g. [optimization](http://docs.scipy.org/doc/scipy/reference/optimize.html), [statistics](http://docs.scipy.org/doc/scipy/reference/stats.html), [interpolation](http://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html), [signal processing](http://docs.scipy.org/doc/scipy/reference/tutorial/signal.html), [sparse matrices](http://docs.scipy.org/doc/scipy/reference/sparse.html), [advanced linear algebra](http://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RidgeRegressionOptimize(RidgeRegressionGradient):\n",
    "    \n",
    "    def __init__(self, alpha=0, method=None):\n",
    "        \"\"\"Here's a new hyper-parameter: the optimization algorithm.\"\"\"\n",
    "        super().__init__(alpha)\n",
    "        self.method = method\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fitted with a general purpose optimization algorithm.\"\"\"\n",
    "        n, d = X.shape\n",
    "        self.b = np.mean(y)\n",
    "        \n",
    "        # Objective and gradient w.r.t. the variable to be optimized.\n",
    "        f = lambda w: self.loss(X, y, w)\n",
    "        jac = lambda w: self.grad(X, y, w)\n",
    "        \n",
    "        # Solve the problem !\n",
    "        from scipy.optimize import minimize\n",
    "        w0 = np.random.normal(size=d)\n",
    "        res = minimize(f, w0, method=self.method, jac=jac)\n",
    "        self.w = res.x\n",
    "\n",
    "models.append(evaluate(RidgeRegressionOptimize(alpha, method='Nelder-Mead')))\n",
    "models.append(evaluate(RidgeRegressionOptimize(alpha, method='BFGS')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy may be lower (depending on the random initialization) as the optimization may not have converged to the global minima. Training time is however much longer ! Especially for gradient-less optimizers such as Nelder-Mead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 More interactivity\n",
    "\n",
    "Interlude: the interactivity of Jupyter notebooks can be pushed forward with [IPython widgets](https://ipywidgets.readthedocs.io). Below, we construct a slider for the model hyper-parameter $\\alpha$, which will train the model and print its performance at each change of the value. Handy when exploring the effects of hyper-parameters ! Although it's less usefull if the required computations are long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "slider = ipywidgets.widgets.FloatSlider(\n",
    "    value=-2,\n",
    "    min=-4,\n",
    "    max=2,\n",
    "    step=1,\n",
    "    description='log(alpha) / n',\n",
    ")\n",
    "\n",
    "def handle(change):\n",
    "    \"\"\"Handler for value change: fit model and print performance.\"\"\"\n",
    "    value = change['new']\n",
    "    alpha = np.power(10, value) * n\n",
    "    clear_output()\n",
    "    print('alpha = {:.2e}'.format(alpha))\n",
    "    evaluate(RidgeRegression(alpha))\n",
    "\n",
    "slider.observe(handle, names='value')\n",
    "display(slider)\n",
    "\n",
    "slider.value = 1  # As if someone moved the slider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Machine Learning made easier\n",
    "\n",
    "Tired of writing algorithms ? Try [scikit-learn](http://scikit-learn.org), which provides many ML algorithms and related tools, e.g. metrics, cross-validation, model selection, feature extraction, pre-processing, for [predictive modeling](https://en.wikipedia.org/wiki/Predictive_modelling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model, metrics\n",
    "\n",
    "# The previously developed model: Ridge Regression.\n",
    "model = linear_model.RidgeClassifier(alpha)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "models.append(model)\n",
    "\n",
    "# Evaluate the predictions with a metric: the classification accuracy.\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print('accuracy: {:.2f}%'.format(acc*100))\n",
    "\n",
    "# It does indeed learn the same parameters.\n",
    "assert np.allclose(models[-1].coef_, models[0].w, rtol=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's try another model !\n",
    "models.append(linear_model.LogisticRegression())\n",
    "models[-1].fit(X_train, y_train)\n",
    "acc = models[-1].score(X_test, y_test)\n",
    "print('accuracy: {:.2f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 Traditional Statistics\n",
    "\n",
    "[Statsmodels](http://statsmodels.sourceforge.net/) is similar to scikit-learn, with much stronger emphasis on parameter estimation and (statistical) testing. It is similar in spirit to other statistical packages such as [R](https://www.r-project.org), [SPSS](http://www.ibm.com/analytics/us/en/technology/spss), [SAS](http://www.sas.com/de_ch/home.html) and [Stata](http://www.stata.com). That split reflects the [two statistical modeling cultures](http://projecteuclid.org/euclid.ss/1009213726): (1) Statistics, which want to know how well a given model fits the data, and what variables \"explain\" or affect the outcome, and (2) Machine Learning, where the main supported task is chosing the \"best\" model for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit the Ordinary Least Square regression model.\n",
    "results = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Inspect the results.\n",
    "print(results.summary())\n",
    "\n",
    "# Yeah, it's the same as scikit-learn.\n",
    "assert np.allclose(results.params, linear_model.Ridge(fit_intercept=False).fit(X_train, y_train).coef_, atol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Deep Learning (DL)\n",
    "\n",
    "Of course ! We got two low-level Python libraries: (1) [TensorFlow](https://www.tensorflow.org/) and (2) [Theano](http://deeplearning.net/software/theano/). Both of them treat data as tensors and construct a computational graph ([dataflow paradigm](https://en.wikipedia.org/wiki/Dataflow_programming)), composed of any mathematical expressions, that get evaluated on CPUs or GPUs. Theano is the pioneer and features an optimizing compiler which will turn the computational graph into efficient code. TensorFlow has a cleaner API (not need to define expressions as strings) and does not require a compilation step (which is painful when developing models).\n",
    "\n",
    "While you'll only use Theano / TensorFlow to develop DL models, these are the higher-level libraries you'll use to define and test DL architectures on your problem:\n",
    "* [Keras](https://keras.io/): TensorFlow & Theano backends\n",
    "* [Lasagne](http://lasagne.readthedocs.io): Theano backend\n",
    "* [nolearn](https://github.com/dnouri/nolearn): sklearn-like abstraction of Lasagne\n",
    "* [Blocks](http://blocks.readthedocs.io): Theano backend\n",
    "* [TFLearn](http://tflearn.org): TensorFlow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class NeuralNet(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Define Neural Network architecture.\"\"\"\n",
    "        self.model = keras.models.Sequential()\n",
    "        self.model.add(keras.layers.Dense(output_dim=46, input_dim=23, activation='relu'))\n",
    "        self.model.add(keras.layers.Dense(output_dim=1, activation='sigmoid'))\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = y / 2 + 0.5  # [-1,1] -> [0,1]\n",
    "        self.model.fit(X, y, nb_epoch=5, batch_size=32)\n",
    "\n",
    "    def predict(self, X):\n",
    "        classes = self.model.predict_classes(X, batch_size=32)\n",
    "        return classes[:,0] * 2 - 1\n",
    "        \n",
    "models.append(NeuralNet())\n",
    "models[-1].fit(X_train, y_train)\n",
    "\n",
    "loss_acc = models[-1].model.evaluate(X_test, y_test/2+0.5, batch_size=32)\n",
    "print('\\n\\nTesting set: {}'.format(loss_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 Evaluation\n",
    "\n",
    "Now that we tried several predictive models, it is time to evaluate them with our chosen metrics and choose the one best suited to our particular problem. Let's plot the *classification accuracy* and the *prediction time* for each classifier with [matplotlib](http://matplotlib.org), the goto 2D plotting library for scientific Python. Its API is similar to matlab.\n",
    "\n",
    "Result: The NeuralNet gives the best accuracy, by a small margin over the much simple logistic regression, but is the slowest method. Which to choose ? Again, it depends on your priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "# Or notebook for interaction.\n",
    "\n",
    "names, acc, times = [], [], []\n",
    "for model in models:\n",
    "    import time\n",
    "    t = time.process_time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    times.append((time.process_time()-t) * 1000)\n",
    "    acc.append(accuracy(y_pred, y_test) * 100)\n",
    "    names.append(type(model).__name__)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(121)\n",
    "plt.plot(acc, '.', markersize=20)\n",
    "plt.title('Accuracy [%]')\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(times, '.', markersize=20)\n",
    "plt.title('Prediction time [ms]')\n",
    "plt.xticks(range(len(names)), names, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13 Big Data\n",
    "\n",
    "TODO: MapReduce, Cluster Computing (Hadoop, Spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 Graph and Networks\n",
    "\n",
    "TODO: networkx, graph-tool, gephi, graphviz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
